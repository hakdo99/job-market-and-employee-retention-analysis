# *************************************
# Created by: Kukpyo (Andrew) Han  - kha107@sfu.ca
# Created on: March 15, 2022
# Last Updated on: April 1, 2022
# Objective: This python code is intended to read the integrated parquet files generated by "jobpost_integrator.py"
#            and apply data cleaning/transformations before they can get pushed to MongoDB.
# Input files (before processing): located under "/integrated_parquet"
# Input files (after processing): located under "/integrated_parquet/archive"
# Output files : located under "/postprocessing_parquet"
# *************************************


# pip or conda to install pyarrow
# pip or conda to install fastparquet

import pandas as pd
import numpy as np
import glob
import os
import re
from datetime import datetime

province_dict = {'British Columbia': 'BC', 'Ontario': 'ON', 'Quebec Provce': 'QC', 'Alberta': 'AB',
                 'Manitoba': 'MB', 'Newfoundland and Labrador': 'NL', 'Prince Edward Island': 'PE',
                 'Nova Scotia': 'NS', 'New Brunswick': 'NB', 'Quebec': 'QC', 'Saskatchewan': 'SK',
                 'Yukon': 'YT', 'Northwest Territories': 'NT', 'Nunavut': 'NU'
                 }

# Glassdoor job posts do not include the province. Add a new city into the appropriate list.
bc_cities = ['Burnaby', 'Richmond', 'Langley', 'Strathcona', 'Vancouver', 'North Vancouver', 'Courtenay',
             'Chetwynd', 'Maple Ridge', 'Surrey', 'Victoria', 'Kelowna', 'Vernon', 'Penticton', 'Delta',
             'Duncan', 'Coquitlam', 'Qualicum Beach', 'New Westminster', 'Abbotsford', 'Cranbrook',
             'Prince George', 'Nanaimo', 'Fort St. John', 'Ladysmith', 'West Vancouver', 'Parksville',
             'Crofton', 'White Rock', 'Pitt Meadows']
on_cities = ['Brampton', 'Mississauga', 'Toronto', 'Ottawa', 'Oakville', 'Scarborough', 'London', 'Kingston',
             'North York', 'Kenora', 'Listowel', 'Thunder Bay', 'Parry Sound', 'Markham', 'Hamilton',
             'North Bay', 'Midtown Toronto', 'Windsor', 'Burlington', 'Milton', 'Woodbridge', 'Brantford',
             'St. Catharines', 'Delhi', 'Vaughan', 'Utterson', 'Sarnia', 'Wyoming', 'Kanata', 'Trenton',
             'Waterloo', 'Richmond Hill', 'Barrie', 'Etobicoke', 'Dundas', 'Swastika', 'Don Mills', 'Guelph',
             'Wheatley', 'Concord', 'Bolton', 'Paris', 'Leamington', 'Cambridge', 'Kitchener', 'Midland',
             'Niagara', 'Greater Sudbury', 'Tillsonburg', 'Bolton', 'Alliston', 'Owen Sound', 'Pickering',
             'Georgetown', 'Timmins', 'Newmarket', 'Cornwall', 'Maple', 'Wiarton', 'Niagara Falls', 'Caledon',
             'Lively', 'Aurora']
ab_cities = ['Calgary', 'Edmonton', 'Okotoks', 'Grande Prairie', 'Leduc', 'Barrhead', 'Edson', 'Canmore',
             'Nisku', 'Suffield', 'Lethbridge', 'Red Deer', 'Vermilion']
qc_cities = ['Quebec', 'Montreal', 'Brossard', 'Anjou', 'Lachine', 'Blainville', 'Saint-Jean-sur-Richelieu',
             'Vaudreuil-Dorion', 'Laval', 'Saint-Louis', 'Dorval', 'Pincourt', 'Saguenay', 'Longueuil',
             'Saint-Constant', 'Mont-Royal', 'Boucherville', 'Lasalle', 'Saint-Hubert', 'Saint-Laurent']
mb_cities = ['Winnipeg', 'Regina', 'Cornwallis']
nl_cities = ['Buchans', "St. John's"]
pe_cities = ['Charlottetown', 'Summerside', 'New Annan']
ns_cities = ['Middle Musquodoboit', 'Halifax', 'Port Hastings', 'Bedford']
nb_cities = ['Fredericton', 'Moncton', 'Saint John']
sk_cities = ['Saskatoon', 'Swift Current', 'Clavet', 'Estevan', 'Broderick', 'Welwyn']
yt_cities = ['Whitehorse']
nt_cities = ['Yellowknife']

# This function is intended to standardize salary info across different datasets.
def standardize_expected_sal(x):
    if x is None:
        pass   # Do nothing
    elif x.startswith('Estimated:'):
        x = x.replace('Estimated:', '').strip()
    elif x == 'None':
        x = np.NaN
    elif x.endswith('/yr (est.)'):
        x = x.replace('/yr (est.)', 'a year').strip()
    elif x.endswith('/hr (est.)'):
        x = x.replace('/hr (est.)', 'an hour').strip()
    elif x.endswith('/mo (est.)'):
        x = x.replace('/mo (est.)', 'a month').strip()

    return x


# This function is intended to standardize location info across different datasets.
def standardize_location(x):

    if x is None:
        pass  # Do nothing
    else:
        # Remove 'Hybrid remote' from the string
        if x.startswith('Hybrid remote'):
            x = x.replace('Hybrid remote:', '').strip()

        # Replace full province names to their corresponding abbreviations.
        for full_name, abbr in province_dict.items():
            x = x.replace(full_name, abbr).strip()

        # When there is Canada as part of the location string (e.g. British Columbia, Canada) then remove it.
        if x.endswith('Canada'):
            if x != 'Canada':
                x = x.replace(', Canada', '').strip()

        # These are to handle naming conventions used in LinkedIn
        if x.startswith('Greater Montreal Metropolitan Area'):
            x = 'Montreal, QC'
        if x.startswith('Greater Toronto Area'):
            x = 'Toronto, ON'
        if x.startswith('Greater Ottawa Metropolitan Area'):
            x = 'Ottawa, ON'
        if x.startswith('Greater Calgary Metropolitan Area'):
            x = 'Calgary, AB'
        if x.startswith('Greater Edmonton Metropolitan Area'):
            x = 'Edmonton, AB'
        if x.startswith('Greater Vancouver Metropolitan Area'):
            x = 'Vancouver, BC'
        if x.startswith('Greater Trois-Rivieres Metropolitan Area'):
            x = 'Trois-Rivieres, QC'
        if x.startswith('Greater Quebec City Metropolitan Area'):
            x = 'Quebec City, QC'
        if x.startswith('Greater Guelph Metropolitan Area'):
            x = 'Guelph, ON'
        if x.startswith('Greater Kitchener-Cambridge-Waterloo Metropolitan Area'):
            x = 'Kitchener-Cambridge-Waterloo, ON'
        if x.startswith('Greater Sherbrooke Metropolitan Area'):
            x = 'Sherbrooke, QC'
        if x.startswith('Greater Victoria Metropolitan Area'):
            x = 'Victoria, BC'
        if x.startswith('Greater Barrie Metropolitan Area'):
            x = 'Barrie, ON'
        if x.startswith('Greater Hamilton (Burlington) Metropolitan Area'):
            x = 'Hamilton, ON'

        if x.startswith('City of'):
            x = x.replace('City of', '').strip()

        # Special handling for locations scraped from Glassdoor
        if x in bc_cities:
            x = x + ", BC"
        elif x in on_cities:
            x = x + ", ON"
        elif x in ab_cities:
            x = x + ", AB"
        elif x in qc_cities:
            x = x + ", QC"
        elif x in mb_cities:
            x = x + ", MB"
        elif x in nl_cities:
            x = x + ", NL"
        elif x in pe_cities:
            x = x + ", PE"
        elif x in ns_cities:
            x = x + ", NS"
        elif x in nb_cities:
            x = x + ", NB"
        elif x in sk_cities:
            x = x + ", SK"
        elif x in yt_cities:
            x = x + ", YT"
        elif x in nt_cities:
            x = x + ", NT"

    return x



if __name__ == "__main__":

    INPUT_PATH = r'integrated_parquet/'
    OUTPUT_PATH = r'postprocessing_parquet/'
    all_files = glob.glob(INPUT_PATH + "*.parquet")
    TIMESTAMP = datetime.today().strftime("%Y%m%d%H")

    # li is used to hold dataframes taken from the integrated_parquet folder.
    li = []
    # li_cleaned is used to hold cleaned dataframes.
    li_cleaned = []

    print('============================== Read in the parquet files ===================================')

    for filename in all_files:
        df = pd.read_parquet(filename, columns=None)
        li.append(df)

    print('============================== Read in the parquet files (completed) ===================================')

    print('============================== Perform cleaning over the datasets ===================================')

    for df_to_process in li:
        df_to_process['expected_salary'] = df_to_process['expected_salary'].apply(standardize_expected_sal)
        df_to_process['location'] = df_to_process['location'].apply(standardize_location)
        # Convert all the values to lowercase for standardization
        df_to_process['remote'] = df_to_process['remote'].apply(lambda a: a.lower() if isinstance(a, str) else a)

        # Special handling for remote jobs scraped from Workopolis
        # When "Remote" is present under location column, convert it to
        df_to_process.loc[df_to_process["location"] == "Remote", "remote"] = "remote"
        df_to_process.loc[df_to_process["location"] == "Remote", "location"] = "Canada"

        # If there is the word "remote" present in the job title, update the remote field.
        df_to_process.loc[df_to_process["location"].str.contains("remote", case=False, na=False), "remote"] = "remote"

        # Append the cleaned dataframes to the python list.
        li_cleaned.append(df_to_process)


    print('============================== Perform cleaning over the datasets (completed) ======================='
          '============')

    result_df = pd.DataFrame()

    for cur_df in li_cleaned:
        if result_df.empty:
            result_df = cur_df
        else:
            # Combine all the dataframes resursively.
            result_df = pd.concat([result_df, cur_df], ignore_index=True)

    result_df.to_parquet(OUTPUT_PATH + "cleaned_" + TIMESTAMP + ".parquet")
    result_df.to_csv(OUTPUT_PATH + "cleaned_" + TIMESTAMP + ".csv")
    print('The parquet file containing the cleaned dataset has been successfully saved.')

    print('============================== Move the processed files to archive ===================================')

    # Move the processed parquet files to archive folder.
    for filename in all_files:
        # Extract the file name from the full path.
        filename_tmp1 = filename.split("\\")
        # Source file path
        source = filename
        # destination file path
        dest = INPUT_PATH + "archive/" + filename_tmp1[1]

        try:
            os.rename(source, dest)
            print(f"{filename_tmp1[1]} : Source path renamed to destination path successfully.")
        # For permission related errors
        except PermissionError:
            print("Operation not permitted.")
        # For other errors
        except OSError as error:
            print(error)

    print('============================== Move the processed files to archive (completed) ====================='
          '=============')
