# -*- coding: utf-8 -*-
"""workopolis_jobs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JpB4ucsQKuWPIKHQNM8Aefn7Dpr3ywvw
"""

from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import random
import csv
import requests
import pandas as pd
import time
import re
import sys

id, title, job_exp, job_type, industries, company, location, source, search_kw = [], [], [], [], [],[], [], [], [],
job_function, estsal, posted, remote, job_summary, description = [],[],[], [], [], [],

def get_result(url, input):
  r = requests.get(url)
  soup = BeautifulSoup(r.content, 'html.parser')
  #finding each article tag that has the job post listed
  articles = soup.find_all('article')
  #find the next page
  page_next = soup.find('a', class_="Pagination-link Pagination-link--next")


  for page in range(1):
    for i in articles:
      #getting data for each column and appending to the list
      title.append((i.find('h2')).text)
      search_kw.append(input)
      id.append('')
      job_exp.append('')
      job_function.append('')
      industries.append('')
      remote.append('')
      # company.append((i.find('div', class_="JobCard-property JobCard-company")).text)

      source.append("Workopolis")   

      comp = i.find('div', class_="JobCard-property JobCard-company")
      if comp:
        company.append(comp.text)


        loc = i.find('span', class_="JobCard-property JobCard-location")
        if loc is None:
          location.append("NA")
        elif loc.text:
          replaced=(loc.text).replace('\xa0â€”\xa0', '')
          location.append(replaced)

        salary = i.find('span', class_="Salary")
        if salary:
          estsal.append(salary.text)
        elif salary is None:
          estsal.append("None")

        #converting to datetime format
        if (i.find('time')):
          jobs=i.find('time', {'class':'JobCard-property JobCard-age'})
          if jobs:
            job_post = jobs ['datetime']

            if job_post:
              date_time = job_post.split('T')
              time = date_time[1][:-1]
              posted.append(datetime.fromisoformat(date_time[0]))
            else:
              posted.append(datetime.now().strftime('%Y-%m-%d'))

          else:
            posted.append(datetime.now().strftime('%Y-%m-%d'))


        else:      
          posted.append(datetime.now().strftime('%Y-%m-%d'))


        summary = i.find('div', class_="JobCard-snippet")
        if summary:
          job_summary.append(summary.text) 
        elif summary is None:
          job_summary.append("")

  
        links = i.find('a', class_="JobCard-titleLink")
        if links:
          full_link = 'https://www.workopolis.com' + links.get('href')
          a = requests.get(full_link)
          soup = BeautifulSoup(a.content, 'html.parser')
          
          f = soup.find('div', class_="viewjob-description ViewJob-description")
          if f:
            desc = (f.text)
            description.append(desc)

          else:
            description.append("")
        
        else:
          description.append("")

        s = soup.find('p', text= re.compile('Job Types:'))
        if s!=None:
          job_type.append(s.text[11:])
        else:
          job_type.append(None)

  # checking if the next page exist or not
  # if it does link to that page and run the function again
  if page_next != None:
    next_page = page_next.get('href')
    link = 'https://www.workopolis.com' + next_page
    get_result(link, input)



  #creating dataframe
  df = pd.DataFrame(list(zip(id, title, job_type, job_exp, company, industries, location, source, search_kw, estsal, posted, job_function, remote, job_summary, description)), columns=['id', 'job_title', 'job_type','job_exp', 'company', 'industries', 'location', 'source', 'search_kw', 'expected_salary', 'post_date', 'job_function', 'remote','job_summary', 'description'])
  
  return(df)

def get_url(position, location):
    input = position
    """Generate a url based on the input postition and location"""
    if len(position)>1:
      position = position.replace(" ", "+")  

      template = "https://www.workopolis.com/jobsearch/find-jobs?ak={}&l={}"
      url = template.format(position, location)
      print(url)
    return (url)

if __name__ == "__main__":
    # python indeed_job_scrapper.py "data analyst" "Vancouver BC"
    position = sys.argv[1]
    job_location = sys.argv[2]
    timestamp_now = datetime.now().strftime('%Y%m%d')
    url = get_url(position, job_location)
    df = get_result(url, position)
    FOLDER = "preprocessing_parquet/"
    df.to_parquet(FOLDER + "Workopolis_" + position + "_" + job_location + "_" + str(timestamp_now) + ".parquet",  index=False)
